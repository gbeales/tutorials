{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\r\n",
    "Neural networks -- 'layers' of operations.  Basically each layer is a model, with outputs of one layer acting as inputs to the next.\r\n",
    "\r\n",
    "`torch.nn` namespace provides all the building blocks you need to build your own neural network.  Every module in PyTorch subclasses the `nn.Module`.  A neural network is a module itself that consists of other modules (layers).  This nested structure allows for building and managing complex architectures easily.\r\n",
    "\r\n",
    "Will use a NN to classify the FashionMNIST dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Device for Training\r\n",
    "We want to be able to use the GPU if available, so check if it's available and set 'device' to gpu if so, else cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "print(f'Using {device} device.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NN class\r\n",
    "We define the neural network by subclassing `nn.Module`, and initialize the layers in `__init__`.  Every `nn.Module` subclass implements the operations on input data in hte `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, move it to `device`, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\r\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this model, we pass the input data directly to the object.  This executes the model's `forward` along with some background operations.  __Do not call `model.forward()`__ directly!\r\n",
    "\r\n",
    "Calling hte model on the input returns a 10-dimensional tensor wit raw predicted values for each class.  We get the prediction probabilities by passin it through an instance of the `nn.Softmax` module (why not just divide by the number of objects?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\r\n",
    "logits = model(X)\r\n",
    "pred_probab = nn.Softmax(dim=1)(logits) #huh?\r\n",
    "y_pred = pred_probab.argmax(1)\r\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers\r\n",
    "Let's break down the layers in the FashionMNIST model.  To illustrate it, we take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\r\n",
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten\r\n",
    "We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (minibatch dimension @dim=0 is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\r\n",
    "flat_image = flatten(input_image)\r\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\r\n",
    "The `linear layer` is a module that applies a linear transformation on the input using its stored weights and biases.\r\n",
    "\r\n",
    "Note that here we go to 20 features in the hidden layer, rather than 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\r\n",
    "hidden1 = layer1(flat_image)\r\n",
    "print(hidden1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.RelU\r\n",
    "Non-linear activations are what create the complex mappings between inputs and outputs (linear ones can, too!).  They are applied after the linear transformations to help neural networks learn non-linear phenomena.\r\n",
    "\r\n",
    "In this model we use `nn.ReLU` -- basically if the input is less than zero the output is 0, otherwise the input is the output -- between our linear layers, but there's other activations to introduce non-linearity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.6389, -0.0292,  0.1809,  0.0834,  0.4015,  0.1838,  0.2065,  0.5643,\n",
      "          0.4094, -0.0710, -0.1800,  0.0794,  0.0536, -0.0173, -0.6627, -0.4817,\n",
      "         -0.2941, -0.1998, -0.6658,  0.0839],\n",
      "        [ 0.4250,  0.0768,  0.0630, -0.1700,  0.6600, -0.0438,  0.5047,  0.1025,\n",
      "          0.2629, -0.3986, -0.3077, -0.3239,  0.5094, -0.1489, -0.2294, -0.4071,\n",
      "         -0.1764, -0.3255, -0.5530,  0.5317],\n",
      "        [ 0.4202, -0.1688,  0.1328, -0.1579,  0.6434,  0.0483,  0.4223,  0.5331,\n",
      "          0.0558, -0.2201, -0.4015, -0.2045,  0.3494, -0.0631, -0.4509, -0.4798,\n",
      "          0.1360, -0.4389, -0.4496,  0.0095]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.6389, 0.0000, 0.1809, 0.0834, 0.4015, 0.1838, 0.2065, 0.5643, 0.4094,\n",
      "         0.0000, 0.0000, 0.0794, 0.0536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0839],\n",
      "        [0.4250, 0.0768, 0.0630, 0.0000, 0.6600, 0.0000, 0.5047, 0.1025, 0.2629,\n",
      "         0.0000, 0.0000, 0.0000, 0.5094, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5317],\n",
      "        [0.4202, 0.0000, 0.1328, 0.0000, 0.6434, 0.0483, 0.4223, 0.5331, 0.0558,\n",
      "         0.0000, 0.0000, 0.0000, 0.3494, 0.0000, 0.0000, 0.0000, 0.1360, 0.0000,\n",
      "         0.0000, 0.0095]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Before ReLU: {hidden1}\\n\\n')\r\n",
    "hidden1 = nn.ReLU()(hidden1)\r\n",
    "print(f'After ReLU: {hidden1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\r\n",
    "`nn.Sequential` is an ordered container of modules.  The data is passed through all the modules in the same order as defined.  You can use sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\r\n",
    "    flatten,\r\n",
    "    layer1,\r\n",
    "    nn.ReLU(),\r\n",
    "    nn.Linear(20, 10)\r\n",
    ")\r\n",
    "input_image = torch.rand(3, 28, 28)\r\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\r\n",
    "The last linear layer of the neural network returns `logits` -- raw values in $[-\\infty, \\infty]$ -- which are passed to the `nn.Softmax` module.  The logits are scaled to values [0, 1] representing the model's predicted probabilities for each class. `dim` parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\r\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\r\n",
    "Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training.  Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters()` methods.\r\n",
    "\r\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0270, -0.0225,  0.0102,  ..., -0.0032, -0.0114, -0.0020],\n",
      "        [-0.0051, -0.0341, -0.0073,  ...,  0.0330, -0.0321,  0.0310]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0164,  0.0192], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0162,  0.0099,  0.0400,  ...,  0.0088,  0.0083,  0.0340],\n",
      "        [ 0.0342,  0.0042, -0.0130,  ...,  0.0397,  0.0199,  0.0239]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0133, 0.0050], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0387, -0.0140,  0.0347,  ...,  0.0191,  0.0021,  0.0056],\n",
      "        [ 0.0403,  0.0187, -0.0316,  ...,  0.0127,  0.0067,  0.0420]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0180,  0.0067], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model structure: ', model, '\\n\\n')\r\n",
    "\r\n",
    "for name, param in model.named_parameters():\r\n",
    "    print(f'Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a10ce0df86bd47f20d28ed217e21e989a0ba99322e6d590bf6db2a306ca8d1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch_tutorials': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}